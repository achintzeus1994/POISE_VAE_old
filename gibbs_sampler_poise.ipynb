{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "municipal-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "latent_dim1 = 1\n",
    "latent_dim2 = 1\n",
    "batch_size  = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-pastor",
   "metadata": {},
   "source": [
    "To sample from the prior we consider its conditional distributions,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    p(z|z')&\\sim \\mathrm{exp}\\left(-(1-g_{22}z'^2)z^2+(g_{11}z'\\right) \\\\\n",
    "    p(z'|z)&\\sim \\mathrm{exp}\\left(-(1-g_{22}z^2) {z'}^2+(g_{11}z\\right)\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "The variance and mean of p(z|z') is given by,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    \\sigma^2&=\\frac{1}{2(1-g_{22}z'^2)}\\\\\n",
    "    \\mu &=\\sigma^2*(g_{11}z')\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "The variance and mean of p(z'|z) is given by,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    \\sigma'^2&=\\frac{1}{2(1-g_{22}z^2)}\\\\\n",
    "    \\mu' &=\\sigma'^2*(g_{11}z)\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "To implement Gibbs sampling on prior, define the following functions:\n",
    "* var_calc_prior:\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    \\mathrm{Input}&:  z\\, (\\mathrm{or}\\, z'),  g_{22} \\\\\n",
    "    \\mathrm{Output}&: \\sigma^2\\, (\\mathrm{or}\\, \\sigma'^2)\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "* mean_calc_prior:  \n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    \\mathrm{Input}&: z\\, (\\mathrm{or}\\, z'), \\sigma^2\\, (\\mathrm{or}\\, \\sigma'^2), g_{11},  g_{22}\\\\\n",
    "    \\mathrm{Output}&: \\mu\\,(\\mathrm{or}\\, \\mu')\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "The algorithm is as follows:\n",
    "* Initialize z and z'\n",
    "* Iterate 5 times and on each iteration calculate\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    \\sigma^2&= \\mathrm{var\\_calc\\_prior}(z',g_{22})\\\\\n",
    "    \\mu   &= \\mathrm{mu\\_calc\\_prior}(z',\\sigma^2, g_{11},  g_{22})\\\\\n",
    "    z    &= \\mu+\\sqrt{\\sigma^2}\\odot \\epsilon\\\\\n",
    "    \\sigma'^2&= \\mathrm{var\\_calc\\_prior}(z,g_{22})\\\\\n",
    "    \\mu'   &= \\mathrm{mu\\_calc\\_prior}(z,\\sigma'^2, g_{11},  g_{22})\\\\\n",
    "    z' &= \\mu'+\\sqrt{\\sigma'^2}\\odot \\epsilon\n",
    "    \\end{aligned}\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becoming-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gibbs_sampler():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    #Gibbs sampling p(z,z')\n",
    "    def var_calc_prior(self,z,g22):\n",
    "        val   = 2*(1-torch.matmul(torch.square(z),g22))\n",
    "        return torch.reciprocal(val)\n",
    "    def mean_calc_prior(self,z,var,g11):\n",
    "        beta = torch.matmul(z,g11)\n",
    "        return var*beta\n",
    "    def initialize_prior_sample(self,g11,g22):\n",
    "        z_prior  = torch.randn(batch_size,latent_dim1).to(device)        ## For estimating z  in  p(z |z')\n",
    "        zp_prior = torch.randn(batch_size,latent_dim2).to(device)       ## For estimating zp  in p(z'|z)\n",
    "        for i in range(5000):\n",
    "            var      = self.var_calc_prior(zp_prior,torch.transpose(g22,0,1))\n",
    "            mean     = self.mean_calc_prior(zp_prior,var,torch.transpose(g11,0,1))\n",
    "            z_prior  = mean+torch.sqrt(var.float())*torch.randn_like(var)\n",
    "            varp     = self.var_calc_prior(z_prior,g22)\n",
    "            meanp    = self.mean_calc_prior(z_prior,varp,g11)\n",
    "            zp_prior = meanp+torch.sqrt(varp.float())*torch.randn_like(varp)\n",
    "        return z_prior,zp_prior\n",
    "    \n",
    "    def prior_sample(self,z_prior,zp_prior,g11,g22):\n",
    "        for i in range(5):\n",
    "            var      = self.var_calc_prior(zp_prior,torch.transpose(g22,0,1))\n",
    "            mean     = self.mean_calc_prior(zp_prior,var,torch.transpose(g11,0,1))\n",
    "            z_prior  = mean+torch.sqrt(var.float())*torch.randn_like(var)\n",
    "            varp     = self.var_calc_prior(z_prior,g22)\n",
    "            meanp    = self.mean_calc_prior(z_prior,varp,g11)\n",
    "            zp_prior = meanp+torch.sqrt(varp.float())*torch.randn_like(varp)\n",
    "        return z_prior,zp_prior\n",
    "    \n",
    "    #Gibbs sampling q(z,z'|x,x')\n",
    "    def var_calc_posterior(self,z,g22,lambda_2):\n",
    "#         print('z',z.size())\n",
    "#         print('g22',g22.size())\n",
    "#         print('lambda_2',lambda_2.size())\n",
    "        val   = 2*(1-torch.matmul(torch.square(z),g22)-lambda_2)\n",
    "        return torch.reciprocal(val)\n",
    "    def mean_calc_posterior(self,z,var,g11,lambda_1):\n",
    "#         print('z',z.size())\n",
    "#         print('g11',g11.size())\n",
    "#         print('lambda_1',lambda_1.size())        \n",
    "        beta = torch.matmul(z,g11)+lambda_1\n",
    "        return var*beta\n",
    "    def initialize_posterior_sample(self,g11,g22,lambda_1,lambda_2,lambdap_1,lambdap_2):\n",
    "        z_posterior = torch.randn(batch_size,latent_dim1).to(device)       ## For estimating z  in  q(z |z',x)\n",
    "        zp_posterior= torch.randn(batch_size,latent_dim2).to(device)       ## For estimating z' in  q(z'|z,x)\n",
    "        for i in range(5000):\n",
    "            var          = self.var_calc_posterior(zp_posterior,torch.transpose(g22,0,1),lambda_2)\n",
    "            mean         = self.mean_calc_posterior(zp_posterior,var,torch.transpose(g11,0,1),lambda_1)\n",
    "            z_posterior  = mean+torch.sqrt(var.float())*torch.randn_like(var)\n",
    "            varp         = self.var_calc_posterior(z_posterior,g22,lambdap_2)\n",
    "            meanp        = self.mean_calc_posterior(z_posterior,varp,g11,lambdap_1)\n",
    "            zp_posterior = meanp+torch.sqrt(varp.float())*torch.randn_like(varp)\n",
    "        return z_posterior,zp_posterior       \n",
    "    \n",
    "    def posterior_sample(self,z_posterior,zp_posterior,g11,g22,lambda_1,lambda_2,lambdap_1,lambdap_2):\n",
    "        for i in range(5):\n",
    "            var          = self.var_calc_posterior(zp_posterior,torch.transpose(g22,0,1),lambda_2)\n",
    "            mean         = self.mean_calc_posterior(zp_posterior,var,torch.transpose(g11,0,1),lambda_1)\n",
    "            z_posterior  = mean+torch.sqrt(var.float())*torch.randn_like(var)\n",
    "            varp         = self.var_calc_posterior(z_posterior,g22,lambdap_2)\n",
    "            meanp        = self.mean_calc_posterior(z_posterior,varp,g11,lambdap_1)\n",
    "            zp_posterior = meanp+torch.sqrt(varp.float())*torch.randn_like(varp)\n",
    "        return z_posterior,zp_posterior       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-launch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
